{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1482d0b",
   "metadata": {},
   "source": [
    "## MLOps with Azure ML Pipelines\n",
    "\n",
    "ML Pipeline - Training & Registration.  ML Pipelines can help you to build, optimize and manage your machine learning workflow. \n",
    "\n",
    "ML Pipelines encapsulate a workflow for a machine learning task.  Tasks often include:\n",
    "- Data Prep\n",
    "- Training \n",
    "- Publishing Models\n",
    "- Deployment of Models\n",
    "\n",
    "First we will set some key variables to be leveraged inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ed2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_env_name = \"experiment_env\"\n",
    "experiment_folder = 'exp_train_pipeline'\n",
    "dataset_prefix_name = 'exp'\n",
    "cluster_name = \"mm-cluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23e496",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8006de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17a44c",
   "metadata": {},
   "source": [
    "### Connect to the workspace and create a cluster for running the AML Pipeline\n",
    "\n",
    "Connect to the AML workspace and the default datastore. To run an AML Pipeline, we will want to create compute if a compute cluster is not already available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f59e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800a4a0",
   "metadata": {},
   "source": [
    "## Create Run configuration\n",
    "\n",
    "The RunConfiguration defines the environment used across all the python steps.  There are a variety of ways of setting up an environment.  An environment holds the required python packages needed for your code to execute on a compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e64e7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c585faf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../configuration/environment.yml\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy file from ../configuration/environment.yml to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14582efd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "142a17dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_train_pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder for the pipeline step files\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1da7b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda_yml_file = '../configuration/environment.yml'\n",
    "conda_yml_file = './'+ experiment_folder+ '/environment.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62d32e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./exp_train_pipeline/environment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $conda_yml_file\n",
    "name: experiment_env\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow\n",
    "  - azureml-monitoring\n",
    "  - azureml-interpret\n",
    "  - inference-schema\n",
    "  - joblib\n",
    "  - azure-ml-api-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03b44524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "env = Environment.from_conda_specification(\"experiment_env\", conda_yml_file)\n",
    "\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = env\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af1db661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment_env'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e7778ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "experiment_env = Environment.from_conda_specification(registered_env_name, conda_yml_file)\n",
    "\n",
    "# Register the environment \n",
    "experiment_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, registered_env_name)\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51624940",
   "metadata": {},
   "source": [
    "## Define Output datasets\n",
    "\n",
    "\n",
    "The **OutputFileDatasetConfig** object is a special kind of data reference that is used for interim storage locations that can be passed between pipeline steps, so you'll create one and use at as the output for the first step and the input for the second step. Note that you need to pass it as a script argument so your code can access the datastore location referenced by the data reference. \n",
    "\n",
    "Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call.\n",
    "\n",
    "These can be viewed in the Datasets tab directly in the AML Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d20c4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from storage location and save to exp_raw_data\n",
    "exp_raw_data       = OutputFileDatasetConfig(name='Exp_Raw_Data', destination=(default_ds, dataset_prefix_name + '_raw_data/{run-id}')).read_delimited_files().register_on_complete(name= dataset_prefix_name + '_Raw_Data')\n",
    "\n",
    "#data split into testing and training\n",
    "exp_training_data  = OutputFileDatasetConfig(name='Exp_Training_Data', destination=(default_ds, dataset_prefix_name + '_training_data/{run-id}')).read_delimited_files().register_on_complete(name=dataset_prefix_name + '_Training_Data')\n",
    "exp_testing_data   = OutputFileDatasetConfig(name='Exp_Testing_Data', destination=(default_ds, dataset_prefix_name + '_testing_data/{run-id}')).read_delimited_files().register_on_complete(name=dataset_prefix_name + '_Testing_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22765c",
   "metadata": {},
   "source": [
    "## Define Pipeline Data\n",
    "\n",
    "Data used in pipeline can be **produced by one step** and **consumed in another step** by providing a PipelineData object as an output of one step and an input of one or more subsequent steps\n",
    "\n",
    "This can be leveraged for moving a model from one step into another for model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c62ebc",
   "metadata": {},
   "source": [
    "### Create Python Script Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2dd4fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_step = PythonScriptStep(\n",
    "    name='Get Data',\n",
    "    script_name='get_data.py',\n",
    "    arguments =['--exp_raw_data', exp_raw_data],\n",
    "    outputs=[exp_raw_data],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f37944cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./exp_train_pipeline/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$experiment_folder/get_data.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import pandas as pd\n",
    "import os\n",
    "import argparse\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "#Parse input arguments\n",
    "#command-line parsing module \n",
    "parser = argparse.ArgumentParser(\"Get data from and register in AML workspace\")\n",
    "parser.add_argument('--exp_raw_data', dest='exp_raw_data', required=True)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "exp_raw_dataset = args.exp_raw_data\n",
    "\n",
    "#Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "#Connect to default data store\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(ds, 'diabetes-data/*.csv'))\n",
    "\n",
    "raw_df = tab_data_set.to_pandas_dataframe()\n",
    "\n",
    "#Make directory on mounted storage\n",
    "os.makedirs(exp_raw_dataset, exist_ok=True)\n",
    "\n",
    "#this will allow us to register the dataset on completion\n",
    "raw_df.to_csv(os.path.join(exp_raw_dataset, 'exp_raw_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba2388",
   "metadata": {},
   "source": [
    "### Split Data Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3159a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_scale_step = PythonScriptStep(\n",
    "    name='Split  Raw Data',\n",
    "    script_name='split.py',\n",
    "    arguments =['--exp_training_data', exp_training_data,\n",
    "                '--exp_testing_data', exp_testing_data],\n",
    "    inputs=[exp_raw_data.as_input(name='Exp_Raw_Data')],\n",
    "    outputs=[exp_training_data, exp_testing_data],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87df8c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./exp_train_pipeline/split.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$experiment_folder/split.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from numpy.random import seed\n",
    "\n",
    "#Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Split raw data into train/test and scale appropriately\")\n",
    "\n",
    "parser.add_argument('--exp_training_data', dest='exp_training_data', required=True)\n",
    "parser.add_argument('--exp_testing_data', dest='exp_testing_data', required=True)\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "exp_training_data = args.exp_training_data\n",
    "exp_testing_data = args.exp_testing_data\n",
    "\n",
    "\n",
    "#Get current run\n",
    "current_run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = current_run.experiment.workspace\n",
    "\n",
    "# Read input dataset to pandas dataframe\n",
    "raw_datset = current_run.input_datasets['Exp_Raw_Data']\n",
    "raw_df = raw_datset.to_pandas_dataframe()\n",
    "\n",
    "\n",
    "for col in raw_df.columns:\n",
    "    missing = raw_df[col].isnull()\n",
    "    num_missing = np.sum(missing)\n",
    "    if num_missing > 0:  \n",
    "        raw_df['quality_{}_ismissing'.format(col)] = missing\n",
    "\n",
    "\n",
    "print(raw_df.columns)\n",
    "\n",
    "#Split data into training set and test set\n",
    "df_train, df_test = train_test_split(raw_df, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Save train data to both train and test (reflects the usage pattern in this sample. Note: test/train sets are typically distinct data).\n",
    "os.makedirs(exp_training_data, exist_ok=True)\n",
    "os.makedirs(exp_testing_data, exist_ok=True)\n",
    "\n",
    "df_train.to_csv(os.path.join(exp_training_data, 'exp_training_data.csv'), index=False)\n",
    "df_test.to_csv(os.path.join(exp_testing_data, 'exp_testing_data.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0f0e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TrainingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a62d0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data will be preprocessed and registered as train/test datasets\n",
    "\n",
    "model_file = PipelineData(name='model_file', datastore=default_ds)\n",
    "\n",
    "#by specifying as input, it does not need to be included in the arguments\n",
    "train_model_step = PythonScriptStep(\n",
    "    name='Train',\n",
    "    script_name='train.py',\n",
    "    arguments =['--model_file_output', model_file],\n",
    "    inputs=[\n",
    "            exp_training_data.as_input(name='Exp_Training_Data'),\n",
    "            exp_testing_data.as_input(name='Exp_Testing_Data'),\n",
    "           ],\n",
    "    outputs = [model_file],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8959aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./exp_train_pipeline/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$experiment_folder/train.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.data.datapath import DataPath\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from numpy.random import seed\n",
    "\n",
    "\n",
    "#Parse input arguments\n",
    "parser = argparse.ArgumentParser(\"Train Logistic Regression model\")\n",
    "parser.add_argument('--model_file_output', dest='model_file_output', required=True)\n",
    "\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "model_file_output = args.model_file_output\n",
    "\n",
    "\n",
    "#Get current run\n",
    "run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "# Read input dataset to pandas dataframe\n",
    "X_train_dataset = run.input_datasets['Exp_Training_Data'].to_pandas_dataframe()\n",
    "X_test_dataset = run.input_datasets['Exp_Testing_Data'].to_pandas_dataframe()\n",
    "\n",
    "print(type(X_train_dataset))\n",
    "\n",
    "# Separate features and labels\n",
    "X_train, y_train = X_train_dataset[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, X_train_dataset['Diabetic'].values\n",
    "X_test, y_test   = X_test_dataset[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, X_test_dataset['Diabetic'].values\n",
    "\n",
    "\n",
    "\n",
    "# Set regularization hyperparameter\n",
    "reg = 0.01\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "run.parent.log(name='AUC', value=np.float(auc))\n",
    "run.parent.log(name='Accuracy', value=np.float(acc))\n",
    "\n",
    "# Save the trained model in the outputs folder\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "joblib.dump(value=model, filename='./outputs/diabetes_model_remote.pkl')\n",
    "\n",
    "os.makedirs(model_file_output, exist_ok=True)\n",
    "\n",
    "shutil.copyfile('./outputs/diabetes_model_remote.pkl', os.path.join(model_file_output, 'diabetes_model_remote.pkl'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c9362",
   "metadata": {},
   "source": [
    "### Evaluate Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a5c5b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate and register model here\n",
    "#Compare metrics from current model and register if better than current\n",
    "#best model\n",
    "\n",
    "\n",
    "deploy_file = PipelineData(name='deploy_file', datastore=default_ds)\n",
    "\n",
    "evaluate_and_register_step = PythonScriptStep(\n",
    "    name='Evaluate and Register Model',\n",
    "    script_name='evaluate_and_register.py',\n",
    "    arguments=[\n",
    "        '--model_file', model_file,\n",
    "        '--deploy_file_output', deploy_file,       \n",
    "    ],\n",
    "    inputs=[model_file.as_input('model_file'),\n",
    "            exp_training_data.as_input(name='Exp_Training_Data'),\n",
    "            exp_testing_data.as_input(name='Exp_Testing_Data')\n",
    "           ],\n",
    "    outputs=[ deploy_file],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e96bc0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./exp_train_pipeline/evaluate_and_register.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$experiment_folder/evaluate_and_register.py\n",
    "\n",
    "from azureml.core import Run, Workspace, Datastore, Dataset\n",
    "from azureml.core.model import Model\n",
    "from azureml.data.datapath import DataPath\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import argparse\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "from azureml.interpret import ExplanationClient\n",
    "from azureml.interpret.scoring.scoring_explainer import LinearScoringExplainer, save\n",
    "\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.compute import ComputeTarget, AksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.webservice import Webservice, AksWebservice\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(\"Evaluate model and register if more performant\")\n",
    "\n",
    "parser.add_argument('--model_file', type=str, required=True)\n",
    "parser.add_argument('--deploy_file_output', type=str, help='File passing in pipeline to deploy')\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "deploy_file = args.deploy_file_output\n",
    "model_file = args.model_file\n",
    "\n",
    "def converttypes(df):\n",
    "    cols = df.columns\n",
    "    for c in cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors = 'coerce')\n",
    "\n",
    "    print('data types')\n",
    "    print(df.dtypes)\n",
    "    return df\n",
    "\n",
    "def model_explain():\n",
    "    #load trinning data\n",
    "    X_train_dataset = run.input_datasets['Exp_Training_Data'].to_pandas_dataframe()\n",
    "    X_test_dataset = run.input_datasets['Exp_Testing_Data'].to_pandas_dataframe()\n",
    "    \n",
    "    X_test_dataset = converttypes(X_test_dataset)\n",
    "    X_train_dataset = converttypes(X_train_dataset)\n",
    "    \n",
    "    X_train, y_train = X_train_dataset[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, X_train_dataset['Diabetic'].values\n",
    "    X_test, y_test   = X_test_dataset[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, X_test_dataset['Diabetic'].values\n",
    "\n",
    "    \n",
    "    #load the model\n",
    "    model_list = Model.list(ws, name=model_name, latest=True)\n",
    "    model_path = model_list[0].download(exist_ok=True)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    #https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/explain-model/azure-integration/scoring-time/train_explain.py\n",
    "    # create an explanation client to store the explanation (contrib API)\n",
    "    client = ExplanationClient.from_run(run)\n",
    "\n",
    "    # create an explainer to validate or debug the model\n",
    "    tabular_explainer = TabularExplainer(model,\n",
    "                                         initialization_examples=X_train,\n",
    "                                         features=['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age'],\n",
    "                                         classes=[0, 1])\n",
    "                                         #transformations=transformations)\n",
    "\n",
    "    # explain overall model predictions (global explanation)\n",
    "    # passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "    # more data (e.g. x_train) will likely lead to higher accuracy, but at a time cost\n",
    "    global_explanation = tabular_explainer.explain_global(X_test)\n",
    "\n",
    "    # uploading model explanation data for storage or visualization\n",
    "    comment = 'Global explanation on classification model trained'\n",
    "    client.upload_model_explanation(global_explanation, comment=comment, model_id=model_reg.id)\n",
    "\n",
    "\n",
    "\n",
    "#Get current run\n",
    "run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "#Get default datastore\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "\n",
    "#Get metrics associated with current parent run\n",
    "metrics = run.get_metrics()\n",
    "\n",
    "print('current run metrics')\n",
    "for key in metrics.keys():\n",
    "        print(key, metrics.get(key))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "print('parent run metrics')\n",
    "#Get metrics associated with current parent run\n",
    "metrics = run.parent.get_metrics()\n",
    "\n",
    "for key in metrics.keys():\n",
    "        print(key, metrics.get(key))\n",
    "print('\\n')\n",
    "\n",
    "current_model_AUC = float(metrics['AUC'])\n",
    "current_model_accuracy = float(metrics['Accuracy'])\n",
    "\n",
    "# Get current model from workspace\n",
    "model_name = 'diabetes_model_remote'\n",
    "model_description = 'Diabetes model remote'\n",
    "model_list = Model.list(ws, name=model_name, latest=True)\n",
    "first_registration = len(model_list)==0\n",
    "\n",
    "updated_tags = {'AUC': current_model_AUC}\n",
    "\n",
    "print('updated tags')\n",
    "print(updated_tags)\n",
    "\n",
    "# Copy  training outputs to relative path for registration\n",
    "\n",
    "\n",
    "\n",
    "relative_model_path = 'outputs'\n",
    "run.upload_folder(name=relative_model_path, path=model_file)\n",
    "\n",
    "\n",
    "\n",
    "#If no model exists register the current model\n",
    "if first_registration:\n",
    "    print('First model registration.')\n",
    "    model_reg = run.register_model(model_path='outputs/diabetes_model_remote.pkl', model_name=model_name,\n",
    "                   tags=updated_tags,\n",
    "                   properties={'AUC': current_model_AUC})\n",
    "\n",
    "    #model_explain()\n",
    "else:\n",
    "    #If a model has been registered previously, check to see if current model \n",
    "    #performs better. If so, register it.\n",
    "    print(dir(model_list[0]))\n",
    "    if float(model_list[0].tags['AUC']) < current_model_AUC:\n",
    "        print('New model performs better than existing model. Register it.')\n",
    "\n",
    "        model_reg = run.register_model(model_path='outputs/diabetes_model_remote.pkl', model_name=model_name,\n",
    "                   tags=updated_tags,\n",
    "                   properties={'AUC': current_model_AUC, 'Accuracy': current_model_accuracy})\n",
    "\n",
    "        #model_explain()\n",
    "        \n",
    "        # Output accuracy to file\n",
    "        with open(deploy_file, 'w+') as f:\n",
    "            f.write(('deploy'))\n",
    "    \n",
    "    else:\n",
    "        print('New model does not perform better than existing model. Cancel run.')\n",
    "        \n",
    "        with open(deploy_file, 'w+') as f:\n",
    "            f.write(('no deployment'))\n",
    "            \n",
    "        run.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c1d8c",
   "metadata": {},
   "source": [
    "### Deploy ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9d39e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "359a7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_deploy_pipeline_data = PipelineData(\n",
    "        name='scoring_url_file', \n",
    "        pipeline_output_name='scoring_url_file',\n",
    "        datastore=default_ds,\n",
    "        output_mode='mount',\n",
    "        is_directory=False)\n",
    "\n",
    "aci_service_name = 'diabetes-service-remote-training'\n",
    "registered_model_name = 'diabetes_model_remote'\n",
    "\n",
    "env_name = PipelineParameter(name='environment_name', default_value=registered_env_name)\n",
    "service_name = PipelineParameter(name='service_name', default_value=aci_service_name)\n",
    "model_name = PipelineParameter(name='model_name', default_value=registered_model_name)\n",
    "\n",
    "deploy_test = PythonScriptStep(\n",
    "    name='Deploy to ACI',\n",
    "    script_name='deployACI.py',\n",
    "    arguments=[\n",
    "        '--deploy_file', deploy_file,\n",
    "        '--environment_name', env_name,\n",
    "        '--service_name', service_name,\n",
    "        '--model_name', model_name\n",
    "    ],\n",
    "    inputs=[deploy_file.as_input('deploy_file')\n",
    "    ],\n",
    "    outputs=[exp_deploy_pipeline_data],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f056c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./exp_train_pipeline/deployACI.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$experiment_folder/deployACI.py\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Workspace, Environment\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AciWebservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Deploy arg parser')\n",
    "parser.add_argument('--environment_name', type=str,help='Environment name')\n",
    "parser.add_argument('--service_name', type=str,help='service name')\n",
    "parser.add_argument('--model_name', type=str,help='model name')\n",
    "parser.add_argument('--deploy_file', type=str, help='File storing if model should be deployed')\n",
    "parser.add_argument('--scoring_url_file', type=str, help='File storing the scoring url')\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "environment_name = args.environment_name\n",
    "deploy_file = args.deploy_file\n",
    "scoring_url_file = args.scoring_url_file\n",
    "service_name = args.service_name\n",
    "model_name = args.model_name\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "model = Model(ws, model_name)\n",
    "\n",
    "env = Environment.get(ws, environment_name)\n",
    "\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "\n",
    "\n",
    "# Deploy model\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "            cpu_cores = 1, \n",
    "            memory_gb = 2, \n",
    "            tags = {'model': 'diabetes remote training'},\n",
    "            auth_enabled=True,\n",
    "            enable_app_insights=True,\n",
    "            collect_model_data=True)\n",
    "\n",
    "try:\n",
    "    service = Webservice(ws, name=service_name)\n",
    "    if service:\n",
    "        service.delete()\n",
    "except WebserviceException as e:\n",
    "         print()\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, aci_config)\n",
    "service.wait_for_deployment(True)\n",
    "    \n",
    "\n",
    "# Output scoring url\n",
    "print(service.scoring_uri)\n",
    "with open(scoring_url_file, 'w+') as f:\n",
    "    f.write(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8f7162a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./exp_train_pipeline/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$experiment_folder/score.py\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "from azureml.monitoring import ModelDataCollector\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "#version 2\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    #Print statement for appinsights custom traces:\n",
    "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
    "    # Get the path to the deployed model file and load it\n",
    "    path = os.path.join(Model.get_model_path('diabetes_model_remote'))\n",
    "    \n",
    "    print(path)\n",
    "    model = joblib.load(path)\n",
    "\n",
    "    \n",
    "    global inputs_dc, prediction_dc\n",
    "    inputs_dc = ModelDataCollector(\"best_model\", designation=\"inputs\", feature_names=['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age'])\n",
    "    prediction_dc = ModelDataCollector(\"best_model\", designation=\"predictions\", feature_names=[\"Diabetic\"])\n",
    "\n",
    "\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        # Get the input data as a numpy array\n",
    "        data = np.array(json.loads(raw_data)['data'])\n",
    "        # Get a prediction from the model\n",
    "        predictions = model.predict(data)\n",
    "        print (\"Prediction created\" + time.strftime(\"%H:%M:%S\"))\n",
    "        # Get the corresponding classname for each prediction (0 or 1)\n",
    "        classnames = ['not-diabetic', 'diabetic']\n",
    "        predicted_classes = []\n",
    "        for prediction in predictions:\n",
    "            predicted_classes.append(classnames[prediction])\n",
    "        # Return the predictions as JSON\n",
    "        \n",
    "         # Log the input and output data to appinsights:\n",
    "        info = {\n",
    "            \"input\": raw_data,\n",
    "            \"output\": predicted_classes\n",
    "            }\n",
    "        print(json.dumps(info))\n",
    "        \n",
    "        inputs_dc.collect(data.tolist()) #this call is saving our input data into Azure Blob\n",
    "        prediction_dc.collect(predicted_classes) #this call is saving our prediction data into Azure Blob\n",
    "\n",
    "        \n",
    "        return json.dumps(predicted_classes)\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        print (error + time.strftime(\"%H:%M:%S\"))\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115753b",
   "metadata": {},
   "source": [
    "## Create Pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e261a",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "058aa1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_data_step, split_scale_step, train_model_step, evaluate_and_register_step, deploy_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7d2c85c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step Get Data [a1dfa377][f068c843-eaf8-4a85-ac0d-6df7223d3d89], (This step will run and generate new outputs)\n",
      "Created step Split  Raw Data [849d806c][90375723-79a9-4857-b490-63db88470059], (This step will run and generate new outputs)\n",
      "Created step Train [160d1250][a889e10e-813e-43ed-a3d2-79fb58665e1b], (This step will run and generate new outputs)\n",
      "Created step Evaluate and Register Model [8dbc938f][cbfaa632-8a14-43b2-bc99-e607b1cf9802], (This step will run and generate new outputs)\n",
      "Created step Deploy to ACI [ac372256][3d991ca8-1833-43ca-85f8-a33f88c6028d], (This step will run and generate new outputs)\n",
      "Submitted PipelineRun 5817898b-b946-41e5-adec-bf7240e2d735\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/5817898b-b946-41e5-adec-bf7240e2d735?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev-ops-rg/workspaces/mm-aml-dev-ops&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRunId: 5817898b-b946-41e5-adec-bf7240e2d735\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/5817898b-b946-41e5-adec-bf7240e2d735?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev-ops-rg/workspaces/mm-aml-dev-ops&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "PipelineRun Status: Running\n",
      "\n",
      "\n",
      "StepRunId: f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev-ops-rg/workspaces/mm-aml-dev-ops&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( Get Data ) Status: NotStarted\n",
      "StepRun( Get Data ) Status: Running\n",
      "\n",
      "StepRun(Get Data) Execution Summary\n",
      "====================================\n",
      "StepRun( Get Data ) Status: Finished\n",
      "{'runId': 'f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d', 'target': 'mm-cluster', 'status': 'Completed', 'startTimeUtc': '2022-01-28T22:20:01.30988Z', 'endTimeUtc': '2022-01-28T22:20:28.55317Z', 'services': {}, 'properties': {'ContentSnapshotId': '994c4def-643a-4e6f-bcc0-8236f10ca6b1', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'f068c843-eaf8-4a85-ac0d-6df7223d3d89', 'azureml.moduleName': 'Get Data', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': 'a1dfa377', 'azureml.pipelinerunid': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipeline': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'outputDatasets': [{'identifier': {'savedId': '2f0e2e0d-1f72-4322-9e9b-20494eb60b39', 'registeredId': 'fcb78be0-8371-4f26-a9a0-a50b996e54be', 'registeredVersion': '21'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'Exp_Raw_Data'}, 'dataset': {\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'exp_raw_data/f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\",\n",
      "    \"ParseDelimited\",\n",
      "    \"DropColumns\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"2f0e2e0d-1f72-4322-9e9b-20494eb60b39\",\n",
      "    \"name\": \"exp_Raw_Data\",\n",
      "    \"version\": 21,\n",
      "    \"workspace\": \"Workspace.create(name='mm-aml-dev-ops', subscription_id='5da07161-3770-4a4b-aa43-418cbbb627cf', resource_group='mm-aml-dev-ops-rg')\"\n",
      "  }\n",
      "}}], 'runDefinition': {'script': 'get_data.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--exp_raw_data', 'DatasetOutputConfig:Exp_Raw_Data'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'mm-cluster', 'dataReferences': {}, 'data': {}, 'outputData': {'Exp_Raw_Data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'workspaceblobstore', 'relativePath': 'exp_raw_data/{run-id}'}, 'uri': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'exp_Raw_Data', 'description': None, 'tags': None, 'properties': {'azureml.pipelineRunId': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipelineRun.moduleNodeId': 'a1dfa377', 'azureml.pipelineRun.outputPortName': 'Exp_Raw_Data'}, 'datasetRegistrationOptions': {'additionalTransformation': '{\\n  \"blocks\": [\\n    {\\n      \"id\": \"7fed6c08-c474-4062-a020-d904890a29ac\",\\n      \"type\": \"Microsoft.DPrep.ParseDelimitedBlock\",\\n      \"arguments\": {\\n        \"columnHeadersMode\": 3,\\n        \"fileEncoding\": 0,\\n        \"handleQuotedLineBreaks\": false,\\n        \"preview\": false,\\n        \"separator\": \",\",\\n        \"skipRows\": 0,\\n        \"skipRowsMode\": 0\\n      },\\n      \"localData\": {},\\n      \"isEnabled\": true,\\n      \"name\": null,\\n      \"annotation\": null\\n    },\\n    {\\n      \"id\": \"ca72814b-f20c-42ee-94fd-4f264211bcce\",\\n      \"type\": \"Microsoft.DPrep.DropColumnsBlock\",\\n      \"arguments\": {\\n        \"columns\": {\\n          \"type\": 0,\\n          \"details\": {\\n            \"selectedColumns\": [\\n              \"Path\"\\n            ]\\n          }\\n        }\\n      },\\n      \"localData\": {},\\n      \"isEnabled\": true,\\n      \"name\": null,\\n      \"annotation\": null\\n    }\\n  ],\\n  \"inspectors\": [],\\n  \"meta\": {\\n    \"steps_added\": \"2\"\\n  }\\n}'}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'experiment_env', 'version': '4', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'dependencies': ['python=3.6.2', 'scikit-learn', 'ipykernel', 'matplotlib', 'pandas', 'pip', {'pip': ['azureml-defaults', 'pyarrow', 'azureml-monitoring', 'azureml-interpret', 'inference-schema', 'joblib', 'azure-ml-api-sdk']}], 'name': 'azureml_3599150719ffbb71885ce3276211def7'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211124.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/backgroundProcess.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=idVuDm4xdW7nNyNrh2u9cZDs6QR%2BSbS1xLwBwntCkm8%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A10%3A28Z&se=2022-01-29T06%3A20%3A28Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=erWo0aA2jTKKvWHADsSBGC9VtIi9X4hAhbs55Wy70dA%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A10%3A28Z&se=2022-01-29T06%3A20%3A28Z&sp=r', 'logs/azureml/dataprep/rslex.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d/logs/azureml/dataprep/rslex.log?sv=2019-07-07&sr=b&sig=IckKi4Xx8tADS5hOci1K0Y%2BR9x57EnINKVmXcmJFHS8%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A10%3A28Z&se=2022-01-29T06%3A20%3A28Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=y5pvPxY%2FCLG2dnOJVol5iI8bw6xSYSsz53GapvXlWnc%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A10%3A28Z&se=2022-01-29T06%3A20%3A28Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=g%2BUUO4xBILjpWpv9D%2B8mDsDuQ1IuCW5kjCWE4TS1%2Bbk%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A10%3A28Z&se=2022-01-29T06%3A20%3A28Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.f3a81647-c8ae-4fd4-a913-0c2d94ed4a1d/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=Bapni5RzOicvZEGxQMPqRAPOH2%2BB2iTG%2BXfcaMop8XU%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A10%3A28Z&se=2022-01-29T06%3A20%3A28Z&sp=r'}, 'submittedBy': 'Megan Masanz'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "StepRunId: c1ff1898-7ec5-4e04-a745-297f1497dfd7\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/c1ff1898-7ec5-4e04-a745-297f1497dfd7?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev-ops-rg/workspaces/mm-aml-dev-ops&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( Split  Raw Data ) Status: Running\n",
      "\n",
      "StepRun(Split  Raw Data) Execution Summary\n",
      "===========================================\n",
      "StepRun( Split  Raw Data ) Status: Finished\n",
      "{'runId': 'c1ff1898-7ec5-4e04-a745-297f1497dfd7', 'target': 'mm-cluster', 'status': 'Completed', 'startTimeUtc': '2022-01-28T22:20:37.545355Z', 'endTimeUtc': '2022-01-28T22:21:06.436602Z', 'services': {}, 'properties': {'ContentSnapshotId': '994c4def-643a-4e6f-bcc0-8236f10ca6b1', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '90375723-79a9-4857-b490-63db88470059', 'azureml.moduleName': 'Split  Raw Data', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '849d806c', 'azureml.pipelinerunid': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipeline': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': 'b5afcaae-4d4b-41e3-8c76-ec75e5158d96'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'Exp_Raw_Data', 'mechanism': 'Direct'}}], 'outputDatasets': [{'identifier': {'savedId': '1bbf4dcc-3160-471e-b31e-bb4f8fc2024e', 'registeredId': '82976d17-f5d3-4fe1-9bdf-86122fbab930', 'registeredVersion': '20'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'Exp_Training_Data'}, 'dataset': {\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'exp_training_data/c1ff1898-7ec5-4e04-a745-297f1497dfd7')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\",\n",
      "    \"ParseDelimited\",\n",
      "    \"DropColumns\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"1bbf4dcc-3160-471e-b31e-bb4f8fc2024e\",\n",
      "    \"name\": \"exp_Training_Data\",\n",
      "    \"version\": 20,\n",
      "    \"workspace\": \"Workspace.create(name='mm-aml-dev-ops', subscription_id='5da07161-3770-4a4b-aa43-418cbbb627cf', resource_group='mm-aml-dev-ops-rg')\"\n",
      "  }\n",
      "}}, {'identifier': {'savedId': '5a282ed1-d445-4f10-b8fe-89ed5b11acc5', 'registeredId': 'a051d337-f7a4-460a-9228-238f3cdcf133', 'registeredVersion': '20'}, 'outputType': 'RunOutput', 'outputDetails': {'outputName': 'Exp_Testing_Data'}, 'dataset': {\n",
      "  \"source\": [\n",
      "    \"('workspaceblobstore', 'exp_testing_data/c1ff1898-7ec5-4e04-a745-297f1497dfd7')\"\n",
      "  ],\n",
      "  \"definition\": [\n",
      "    \"GetDatastoreFiles\",\n",
      "    \"ParseDelimited\",\n",
      "    \"DropColumns\"\n",
      "  ],\n",
      "  \"registration\": {\n",
      "    \"id\": \"5a282ed1-d445-4f10-b8fe-89ed5b11acc5\",\n",
      "    \"name\": \"exp_Testing_Data\",\n",
      "    \"version\": 20,\n",
      "    \"workspace\": \"Workspace.create(name='mm-aml-dev-ops', subscription_id='5da07161-3770-4a4b-aa43-418cbbb627cf', resource_group='mm-aml-dev-ops-rg')\"\n",
      "  }\n",
      "}}], 'runDefinition': {'script': 'split.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--exp_training_data', 'DatasetOutputConfig:Exp_Training_Data', '--exp_testing_data', 'DatasetOutputConfig:Exp_Testing_Data'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'mm-cluster', 'dataReferences': {}, 'data': {'Exp_Raw_Data': {'dataLocation': {'dataset': {'id': 'b5afcaae-4d4b-41e3-8c76-ec75e5158d96', 'name': None, 'version': None}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'Exp_Raw_Data', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {'Exp_Training_Data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'workspaceblobstore', 'relativePath': 'exp_training_data/{run-id}'}, 'uri': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'exp_Training_Data', 'description': None, 'tags': None, 'properties': {'azureml.pipelineRunId': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipelineRun.moduleNodeId': '849d806c', 'azureml.pipelineRun.outputPortName': 'Exp_Training_Data'}, 'datasetRegistrationOptions': {'additionalTransformation': '{\\n  \"blocks\": [\\n    {\\n      \"id\": \"db5e9849-f65d-4e63-b7a3-8f1627437330\",\\n      \"type\": \"Microsoft.DPrep.ParseDelimitedBlock\",\\n      \"arguments\": {\\n        \"columnHeadersMode\": 3,\\n        \"fileEncoding\": 0,\\n        \"handleQuotedLineBreaks\": false,\\n        \"preview\": false,\\n        \"separator\": \",\",\\n        \"skipRows\": 0,\\n        \"skipRowsMode\": 0\\n      },\\n      \"localData\": {},\\n      \"isEnabled\": true,\\n      \"name\": null,\\n      \"annotation\": null\\n    },\\n    {\\n      \"id\": \"545a3db7-4a21-4656-ac71-c7060e5e94ab\",\\n      \"type\": \"Microsoft.DPrep.DropColumnsBlock\",\\n      \"arguments\": {\\n        \"columns\": {\\n          \"type\": 0,\\n          \"details\": {\\n            \"selectedColumns\": [\\n              \"Path\"\\n            ]\\n          }\\n        }\\n      },\\n      \"localData\": {},\\n      \"isEnabled\": true,\\n      \"name\": null,\\n      \"annotation\": null\\n    }\\n  ],\\n  \"inspectors\": [],\\n  \"meta\": {\\n    \"steps_added\": \"2\"\\n  }\\n}'}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}, 'Exp_Testing_Data': {'outputLocation': {'dataset': None, 'dataPath': {'datastoreName': 'workspaceblobstore', 'relativePath': 'exp_testing_data/{run-id}'}, 'uri': None}, 'mechanism': 'Mount', 'additionalOptions': {'pathOnCompute': None, 'registrationOptions': {'name': 'exp_Testing_Data', 'description': None, 'tags': None, 'properties': {'azureml.pipelineRunId': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipelineRun.moduleNodeId': '849d806c', 'azureml.pipelineRun.outputPortName': 'Exp_Testing_Data'}, 'datasetRegistrationOptions': {'additionalTransformation': '{\\n  \"blocks\": [\\n    {\\n      \"id\": \"5811debe-d98b-4643-91eb-0e89b6661db5\",\\n      \"type\": \"Microsoft.DPrep.ParseDelimitedBlock\",\\n      \"arguments\": {\\n        \"columnHeadersMode\": 3,\\n        \"fileEncoding\": 0,\\n        \"handleQuotedLineBreaks\": false,\\n        \"preview\": false,\\n        \"separator\": \",\",\\n        \"skipRows\": 0,\\n        \"skipRowsMode\": 0\\n      },\\n      \"localData\": {},\\n      \"isEnabled\": true,\\n      \"name\": null,\\n      \"annotation\": null\\n    },\\n    {\\n      \"id\": \"016a3485-3227-4539-9ca2-a9d13d7b6a24\",\\n      \"type\": \"Microsoft.DPrep.DropColumnsBlock\",\\n      \"arguments\": {\\n        \"columns\": {\\n          \"type\": 0,\\n          \"details\": {\\n            \"selectedColumns\": [\\n              \"Path\"\\n            ]\\n          }\\n        }\\n      },\\n      \"localData\": {},\\n      \"isEnabled\": true,\\n      \"name\": null,\\n      \"annotation\": null\\n    }\\n  ],\\n  \"inspectors\": [],\\n  \"meta\": {\\n    \"steps_added\": \"2\"\\n  }\\n}'}}, 'uploadOptions': {'overwrite': False, 'sourceGlobs': {'globPatterns': None}}, 'mountOptions': None}, 'environmentVariableName': None}}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'experiment_env', 'version': '4', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'dependencies': ['python=3.6.2', 'scikit-learn', 'ipykernel', 'matplotlib', 'pandas', 'pip', {'pip': ['azureml-defaults', 'pyarrow', 'azureml-monitoring', 'azureml-interpret', 'inference-schema', 'joblib', 'azure-ml-api-sdk']}], 'name': 'azureml_3599150719ffbb71885ce3276211def7'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211124.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/backgroundProcess.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.c1ff1898-7ec5-4e04-a745-297f1497dfd7/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=4zWYL3YM53JjJEulRvzBZTYdo9BJ%2BNEUQkX4VlPu0d4%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A06Z&se=2022-01-29T06%3A21%3A06Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.c1ff1898-7ec5-4e04-a745-297f1497dfd7/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=Q%2FESH52hsSR1OLeZlrEHl6GQALn3Iz%2FKj6mhp%2Fead6M%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A06Z&se=2022-01-29T06%3A21%3A06Z&sp=r', 'logs/azureml/dataprep/rslex.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.c1ff1898-7ec5-4e04-a745-297f1497dfd7/logs/azureml/dataprep/rslex.log?sv=2019-07-07&sr=b&sig=7iQ6UPKQ0l73KTeg2KxWQxTmVw3P%2BRYj0Nk0%2FJvqav8%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A06Z&se=2022-01-29T06%3A21%3A06Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.c1ff1898-7ec5-4e04-a745-297f1497dfd7/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=dliNCn%2FUqz3A79%2B9zRmncara301AJgrnE9M7rVRU%2F2M%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A06Z&se=2022-01-29T06%3A21%3A06Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.c1ff1898-7ec5-4e04-a745-297f1497dfd7/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=PiJBZvg7ej%2BUCFo7jETl8ck%2FyCe2liIotoSGGGi9HNw%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A06Z&se=2022-01-29T06%3A21%3A06Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.c1ff1898-7ec5-4e04-a745-297f1497dfd7/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=F3gztJG%2Bj6DVVV2Z3xKndx5GjV0%2F8BdpyPK6MZWzd%2BY%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A06Z&se=2022-01-29T06%3A21%3A06Z&sp=r'}, 'submittedBy': 'Megan Masanz'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "StepRunId: 4ce36af2-5781-41a7-87ac-c92d4da693a5\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/4ce36af2-5781-41a7-87ac-c92d4da693a5?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev-ops-rg/workspaces/mm-aml-dev-ops&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( Train ) Status: NotStarted\n",
      "StepRun( Train ) Status: Running\n",
      "\n",
      "StepRun(Train) Execution Summary\n",
      "=================================\n",
      "StepRun( Train ) Status: Finished\n",
      "{'runId': '4ce36af2-5781-41a7-87ac-c92d4da693a5', 'target': 'mm-cluster', 'status': 'Completed', 'startTimeUtc': '2022-01-28T22:21:18.996953Z', 'endTimeUtc': '2022-01-28T22:21:45.163652Z', 'services': {}, 'properties': {'ContentSnapshotId': '994c4def-643a-4e6f-bcc0-8236f10ca6b1', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'a889e10e-813e-43ed-a3d2-79fb58665e1b', 'azureml.moduleName': 'Train', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '160d1250', 'azureml.pipelinerunid': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipeline': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': 'aaaf32bd-97e8-402c-9a18-87b21a35b2b2'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'Exp_Training_Data', 'mechanism': 'Direct'}}, {'dataset': {'id': 'ee79d81a-760c-4d3c-8cfd-e7a2ca323a20'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'Exp_Testing_Data', 'mechanism': 'Direct'}}], 'outputDatasets': [], 'runDefinition': {'script': 'train.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--model_file_output', '$AZUREML_DATAREFERENCE_model_file'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'mm-cluster', 'dataReferences': {'model_file': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/4ce36af2-5781-41a7-87ac-c92d4da693a5/model_file', 'pathOnCompute': None, 'overwrite': False}}, 'data': {'Exp_Training_Data': {'dataLocation': {'dataset': {'id': 'aaaf32bd-97e8-402c-9a18-87b21a35b2b2', 'name': None, 'version': None}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'Exp_Training_Data', 'pathOnCompute': None, 'overwrite': False, 'options': None}, 'Exp_Testing_Data': {'dataLocation': {'dataset': {'id': 'ee79d81a-760c-4d3c-8cfd-e7a2ca323a20', 'name': None, 'version': None}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'Exp_Testing_Data', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'experiment_env', 'version': '4', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'dependencies': ['python=3.6.2', 'scikit-learn', 'ipykernel', 'matplotlib', 'pandas', 'pip', {'pip': ['azureml-defaults', 'pyarrow', 'azureml-monitoring', 'azureml-interpret', 'inference-schema', 'joblib', 'azure-ml-api-sdk']}], 'name': 'azureml_3599150719ffbb71885ce3276211def7'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211124.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/dataprep/backgroundProcess.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.4ce36af2-5781-41a7-87ac-c92d4da693a5/logs/azureml/dataprep/backgroundProcess.log?sv=2019-07-07&sr=b&sig=NkHxc4CaYEWqAb0XiXOrtxQKhumw9WPF78xaDrPRz3I%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A45Z&se=2022-01-29T06%3A21%3A45Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.4ce36af2-5781-41a7-87ac-c92d4da693a5/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-07-07&sr=b&sig=xq%2FfNAUBaXwCwoYBWRmrdmsBCYnOl6fR8qQ1ytOLgCU%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A45Z&se=2022-01-29T06%3A21%3A45Z&sp=r', 'logs/azureml/dataprep/rslex.log': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.4ce36af2-5781-41a7-87ac-c92d4da693a5/logs/azureml/dataprep/rslex.log?sv=2019-07-07&sr=b&sig=HFgJii7BH5fgH8bIJmDzj99noRCPKf%2BASamcpK336y0%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A45Z&se=2022-01-29T06%3A21%3A45Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.4ce36af2-5781-41a7-87ac-c92d4da693a5/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=z9YRcd3ZoxgIQH8%2Bu6KBtSAJKwCLzYiORKeEtTxFPvM%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A45Z&se=2022-01-29T06%3A21%3A45Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.4ce36af2-5781-41a7-87ac-c92d4da693a5/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=HWnaJhmD9lp6Ap%2FHcxZAKFpZlWtlIF%2FRVKeXiNlmqnc%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A45Z&se=2022-01-29T06%3A21%3A45Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.4ce36af2-5781-41a7-87ac-c92d4da693a5/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=GZMzt2rLEzYiMujyDfyxmpxQP0TrC0omKqekrEYMXW4%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A45Z&se=2022-01-29T06%3A21%3A45Z&sp=r'}, 'submittedBy': 'Megan Masanz'}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "StepRunId: 66fe6dd6-47e2-4907-a13f-0922caa5dd49\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/66fe6dd6-47e2-4907-a13f-0922caa5dd49?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev-ops-rg/workspaces/mm-aml-dev-ops&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( Evaluate and Register Model ) Status: Running\n",
      "\n",
      "StepRun(Evaluate and Register Model) Execution Summary\n",
      "=======================================================\n",
      "StepRun( Evaluate and Register Model ) Status: Finished\n",
      "{'runId': '66fe6dd6-47e2-4907-a13f-0922caa5dd49', 'target': 'mm-cluster', 'status': 'Completed', 'startTimeUtc': '2022-01-28T22:21:57.890408Z', 'endTimeUtc': '2022-01-28T22:22:12.446302Z', 'services': {}, 'properties': {'ContentSnapshotId': '994c4def-643a-4e6f-bcc0-8236f10ca6b1', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': 'cbfaa632-8a14-43b2-bc99-e607b1cf9802', 'azureml.moduleName': 'Evaluate and Register Model', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '8dbc938f', 'azureml.pipelinerunid': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipeline': '5817898b-b946-41e5-adec-bf7240e2d735', 'azureml.pipelineComponent': 'masterescloud', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [{'dataset': {'id': 'aaaf32bd-97e8-402c-9a18-87b21a35b2b2'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'Exp_Training_Data', 'mechanism': 'Direct'}}, {'dataset': {'id': 'ee79d81a-760c-4d3c-8cfd-e7a2ca323a20'}, 'consumptionDetails': {'type': 'RunInput', 'inputName': 'Exp_Testing_Data', 'mechanism': 'Direct'}}], 'outputDatasets': [], 'runDefinition': {'script': 'evaluate_and_register.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--model_file', '$AZUREML_DATAREFERENCE_model_file', '--deploy_file_output', '$AZUREML_DATAREFERENCE_deploy_file'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'mm-cluster', 'dataReferences': {'model_file': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/4ce36af2-5781-41a7-87ac-c92d4da693a5/model_file', 'pathOnCompute': None, 'overwrite': False}, 'deploy_file': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/66fe6dd6-47e2-4907-a13f-0922caa5dd49/deploy_file', 'pathOnCompute': None, 'overwrite': False}}, 'data': {'Exp_Training_Data': {'dataLocation': {'dataset': {'id': 'aaaf32bd-97e8-402c-9a18-87b21a35b2b2', 'name': None, 'version': None}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'Exp_Training_Data', 'pathOnCompute': None, 'overwrite': False, 'options': None}, 'Exp_Testing_Data': {'dataLocation': {'dataset': {'id': 'ee79d81a-760c-4d3c-8cfd-e7a2ca323a20', 'name': None, 'version': None}, 'dataPath': None, 'uri': None}, 'mechanism': 'Direct', 'environmentVariableName': 'Exp_Testing_Data', 'pathOnCompute': None, 'overwrite': False, 'options': None}}, 'outputData': {}, 'datacaches': [], 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'instanceTypes': [], 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'experiment_env', 'version': '4', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'dependencies': ['python=3.6.2', 'scikit-learn', 'ipykernel', 'matplotlib', 'pandas', 'pip', {'pip': ['azureml-defaults', 'pyarrow', 'azureml-monitoring', 'azureml-interpret', 'inference-schema', 'joblib', 'azure-ml-api-sdk']}], 'name': 'azureml_3599150719ffbb71885ce3276211def7'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:20211124.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': False, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': 'D2', 'imageVersion': 'pytorch-1.7.0', 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None, 'sshPublicKeys': None, 'enableAzmlInt': True, 'priority': 'Medium', 'slaTier': 'Standard', 'userAlias': None}, 'kubernetesCompute': {'instanceType': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}, 'parameters': []}, 'logFiles': {'logs/azureml/executionlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.66fe6dd6-47e2-4907-a13f-0922caa5dd49/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=hYpV0iKC%2BKPXH0cNrXsRHvYLykdCVY7bk%2BtO0GUaxyA%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A54Z&se=2022-01-29T06%3A21%3A54Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.66fe6dd6-47e2-4907-a13f-0922caa5dd49/logs/azureml/stderrlogs.txt?sv=2019-07-07&sr=b&sig=H2Pm13HEk2dIEk4tgKh9pTY5xmalN0MMxFdHvCIyUF0%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A54Z&se=2022-01-29T06%3A21%3A54Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://mmamldevops9020263291.blob.core.windows.net/azureml/ExperimentRun/dcid.66fe6dd6-47e2-4907-a13f-0922caa5dd49/logs/azureml/stdoutlogs.txt?sv=2019-07-07&sr=b&sig=4FQ2aXWe1dOjy1FNeVrDLKOLaGosN4ojVQExuLxe1Nw%3D&skoid=6e96e716-19f5-4664-a48c-bccfc5f7e7f7&sktid=72f988bf-86f1-41af-91ab-2d7cd011db47&skt=2022-01-28T15%3A17%3A19Z&ske=2022-01-29T23%3A27%3A19Z&sks=b&skv=2019-07-07&st=2022-01-28T22%3A11%3A54Z&se=2022-01-29T06%3A21%3A54Z&sp=r'}, 'submittedBy': 'Megan Masanz'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StepRunId: 62851a2c-ba3c-45cc-87e4-8bd6f595a895\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/62851a2c-ba3c-45cc-87e4-8bd6f595a895?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev-ops-rg/workspaces/mm-aml-dev-ops&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n",
      "StepRun( Deploy to ACI ) Status: Running\n",
      "\n",
      "StepRun(Deploy to ACI) Execution Summary\n",
      "=========================================\n",
      "StepRun( Deploy to ACI ) Status: Failed\n",
      "\n",
      "Warnings:\n",
      "{\n",
      "  \"error\": {\n",
      "    \"code\": \"UserError\",\n",
      "    \"severity\": null,\n",
      "    \"message\": \"AzureMLCompute job failed.\\nExecutionFailed: [REDACTED]\\n\\texit_codes: 1\",\n",
      "    \"messageFormat\": \"{Message}\",\n",
      "    \"messageParameters\": {\n",
      "      \"Message\": \"AzureMLCompute job failed.\\nExecutionFailed: [REDACTED]\\n\\texit_codes: 1\"\n",
      "    },\n",
      "    \"referenceCode\": null,\n",
      "    \"detailsUri\": null,\n",
      "    \"target\": null,\n",
      "    \"details\": [],\n",
      "    \"innerError\": {\n",
      "      \"code\": \"UserTrainingScriptFailed\",\n",
      "      \"innerError\": null\n",
      "    },\n",
      "    \"debugInfo\": null,\n",
      "    \"additionalInfo\": null\n",
      "  },\n",
      "  \"correlation\": {\n",
      "    \"operation\": \"2eef74199d65764f8121dd1d39716778\",\n",
      "    \"request\": \"9f1e6741fb8b4a23\"\n",
      "  },\n",
      "  \"environment\": \"eastus\",\n",
      "  \"location\": \"eastus\",\n",
      "  \"time\": \"2022-01-28T22:28:54.935415+00:00\",\n",
      "  \"componentName\": \"globaljobdispatcher\"\n",
      "}\n"
     ]
    },
    {
     "ename": "ActivityFailedException",
     "evalue": "ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"{'code': ExecutionFailed, 'message': [{\\\"exit_code\\\":1,\\\"error_message\\\":\\\"Execution failed with error: Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\\\\nRunning\\\\n2022-01-28 22:22:32+00:00 Creating Container Registry if not exists.\\\\n2022-01-28 22:22:33+00:00 Registering the environment.\\\\n2022-01-28 22:22:34+00:00 Generating deployment configuration.\\\\n2022-01-28 22:22:35+00:00 Submitting deployment to compute.\\\\n2022-01-28 22:22:39+00:00 Checking the status of deployment diabetes-service-remote-training..\\\\n2022-01-28 22:28:41+00:00 Checking the status of inference endpoint diabetes-service-remote-training.\\\\nSucceeded\\\\n[stderr]Traceback (most recent call last):\\\\n[stderr]  File \\\\\\\"deployACI.py\\\\\\\", line 61, in <module>\\\\n[stderr]    with open(scoring_url_file, 'w+') as f:\\\\n[stderr]TypeError: expected str, bytes or os.PathLike object, not NoneType\\\\n[stderr]\\\\nACI service creation operation finished, operation \\\\\\\"Succeeded\\\\\\\"\\\\nhttp://d776f11b-defe-49a2-a9b4-f652c488e854.eastus.azurecontainer.io/score\\\\nCleaning up all outstanding Run operations, waiting 300.0 seconds\\\\n5 items cleaning up...\\\\nCleanup took 0.46288228034973145 seconds\\\\n\\\",\\\"process_name\\\":\\\"/azureml-envs/azureml_3599150719ffbb71885ce3276211def7/bin/python\\\",\\\"error_file\\\":\\\"user_logs/std_log.txt\\\"}], 'target': , 'category': UserError, 'error_details': [{'key': exit_codes, 'value': 1}, ], 'inner_error': null}\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"{'code': ExecutionFailed, 'message': [{\\\\\\\"exit_code\\\\\\\":1,\\\\\\\"error_message\\\\\\\":\\\\\\\"Execution failed with error: Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\\\\\\\\nRunning\\\\\\\\n2022-01-28 22:22:32+00:00 Creating Container Registry if not exists.\\\\\\\\n2022-01-28 22:22:33+00:00 Registering the environment.\\\\\\\\n2022-01-28 22:22:34+00:00 Generating deployment configuration.\\\\\\\\n2022-01-28 22:22:35+00:00 Submitting deployment to compute.\\\\\\\\n2022-01-28 22:22:39+00:00 Checking the status of deployment diabetes-service-remote-training..\\\\\\\\n2022-01-28 22:28:41+00:00 Checking the status of inference endpoint diabetes-service-remote-training.\\\\\\\\nSucceeded\\\\\\\\n[stderr]Traceback (most recent call last):\\\\\\\\n[stderr]  File \\\\\\\\\\\\\\\"deployACI.py\\\\\\\\\\\\\\\", line 61, in <module>\\\\\\\\n[stderr]    with open(scoring_url_file, 'w+') as f:\\\\\\\\n[stderr]TypeError: expected str, bytes or os.PathLike object, not NoneType\\\\\\\\n[stderr]\\\\\\\\nACI service creation operation finished, operation \\\\\\\\\\\\\\\"Succeeded\\\\\\\\\\\\\\\"\\\\\\\\nhttp://d776f11b-defe-49a2-a9b4-f652c488e854.eastus.azurecontainer.io/score\\\\\\\\nCleaning up all outstanding Run operations, waiting 300.0 seconds\\\\\\\\n5 items cleaning up...\\\\\\\\nCleanup took 0.46288228034973145 seconds\\\\\\\\n\\\\\\\",\\\\\\\"process_name\\\\\\\":\\\\\\\"/azureml-envs/azureml_3599150719ffbb71885ce3276211def7/bin/python\\\\\\\",\\\\\\\"error_file\\\\\\\":\\\\\\\"user_logs/std_log.txt\\\\\\\"}], 'target': , 'category': UserError, 'error_details': [{'key': exit_codes, 'value': 1}, ], 'inner_error': null}\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mActivityFailedException\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-2da896aa89c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'AML_Automation_RemotePipelineTraining'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubmit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    294\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                                 step_run.wait_for_completion(timeout_seconds=timeout_seconds - time_elapsed,\n\u001b[0;32m--> 296\u001b[0;31m                                                              raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    297\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                                 \u001b[0;31m# If there are package conflicts in the user's environment, the run rehydration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36mwait_for_completion\u001b[0;34m(self, show_output, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 return self._stream_run_output(timeout_seconds=timeout_seconds,\n\u001b[0;32m--> 738\u001b[0;31m                                                raise_on_error=raise_on_error)\n\u001b[0m\u001b[1;32m    739\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m                 \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The output streaming for the run interrupted.\\n\"\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/azureml_py36/lib/python3.6/site-packages/azureml/pipeline/core/run.py\u001b[0m in \u001b[0;36m_stream_run_output\u001b[0;34m(self, timeout_seconds, raise_on_error)\u001b[0m\n\u001b[1;32m    828\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mraise_on_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mActivityFailedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_details\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mActivityFailedException\u001b[0m: ActivityFailedException:\n\tMessage: Activity Failed:\n{\n    \"error\": {\n        \"code\": \"UserError\",\n        \"message\": \"{'code': ExecutionFailed, 'message': [{\\\"exit_code\\\":1,\\\"error_message\\\":\\\"Execution failed with error: Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\\\\nRunning\\\\n2022-01-28 22:22:32+00:00 Creating Container Registry if not exists.\\\\n2022-01-28 22:22:33+00:00 Registering the environment.\\\\n2022-01-28 22:22:34+00:00 Generating deployment configuration.\\\\n2022-01-28 22:22:35+00:00 Submitting deployment to compute.\\\\n2022-01-28 22:22:39+00:00 Checking the status of deployment diabetes-service-remote-training..\\\\n2022-01-28 22:28:41+00:00 Checking the status of inference endpoint diabetes-service-remote-training.\\\\nSucceeded\\\\n[stderr]Traceback (most recent call last):\\\\n[stderr]  File \\\\\\\"deployACI.py\\\\\\\", line 61, in <module>\\\\n[stderr]    with open(scoring_url_file, 'w+') as f:\\\\n[stderr]TypeError: expected str, bytes or os.PathLike object, not NoneType\\\\n[stderr]\\\\nACI service creation operation finished, operation \\\\\\\"Succeeded\\\\\\\"\\\\nhttp://d776f11b-defe-49a2-a9b4-f652c488e854.eastus.azurecontainer.io/score\\\\nCleaning up all outstanding Run operations, waiting 300.0 seconds\\\\n5 items cleaning up...\\\\nCleanup took 0.46288228034973145 seconds\\\\n\\\",\\\"process_name\\\":\\\"/azureml-envs/azureml_3599150719ffbb71885ce3276211def7/bin/python\\\",\\\"error_file\\\":\\\"user_logs/std_log.txt\\\"}], 'target': , 'category': UserError, 'error_details': [{'key': exit_codes, 'value': 1}, ], 'inner_error': null}\",\n        \"messageParameters\": {},\n        \"details\": []\n    },\n    \"time\": \"0001-01-01T00:00:00.000Z\"\n}\n\tInnerException None\n\tErrorResponse \n{\n    \"error\": {\n        \"message\": \"Activity Failed:\\n{\\n    \\\"error\\\": {\\n        \\\"code\\\": \\\"UserError\\\",\\n        \\\"message\\\": \\\"{'code': ExecutionFailed, 'message': [{\\\\\\\"exit_code\\\\\\\":1,\\\\\\\"error_message\\\\\\\":\\\\\\\"Execution failed with error: Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\\\\\\\\nRunning\\\\\\\\n2022-01-28 22:22:32+00:00 Creating Container Registry if not exists.\\\\\\\\n2022-01-28 22:22:33+00:00 Registering the environment.\\\\\\\\n2022-01-28 22:22:34+00:00 Generating deployment configuration.\\\\\\\\n2022-01-28 22:22:35+00:00 Submitting deployment to compute.\\\\\\\\n2022-01-28 22:22:39+00:00 Checking the status of deployment diabetes-service-remote-training..\\\\\\\\n2022-01-28 22:28:41+00:00 Checking the status of inference endpoint diabetes-service-remote-training.\\\\\\\\nSucceeded\\\\\\\\n[stderr]Traceback (most recent call last):\\\\\\\\n[stderr]  File \\\\\\\\\\\\\\\"deployACI.py\\\\\\\\\\\\\\\", line 61, in <module>\\\\\\\\n[stderr]    with open(scoring_url_file, 'w+') as f:\\\\\\\\n[stderr]TypeError: expected str, bytes or os.PathLike object, not NoneType\\\\\\\\n[stderr]\\\\\\\\nACI service creation operation finished, operation \\\\\\\\\\\\\\\"Succeeded\\\\\\\\\\\\\\\"\\\\\\\\nhttp://d776f11b-defe-49a2-a9b4-f652c488e854.eastus.azurecontainer.io/score\\\\\\\\nCleaning up all outstanding Run operations, waiting 300.0 seconds\\\\\\\\n5 items cleaning up...\\\\\\\\nCleanup took 0.46288228034973145 seconds\\\\\\\\n\\\\\\\",\\\\\\\"process_name\\\\\\\":\\\\\\\"/azureml-envs/azureml_3599150719ffbb71885ce3276211def7/bin/python\\\\\\\",\\\\\\\"error_file\\\\\\\":\\\\\\\"user_logs/std_log.txt\\\\\\\"}], 'target': , 'category': UserError, 'error_details': [{'key': exit_codes, 'value': 1}, ], 'inner_error': null}\\\",\\n        \\\"messageParameters\\\": {},\\n        \\\"details\\\": []\\n    },\\n    \\\"time\\\": \\\"0001-01-01T00:00:00.000Z\\\"\\n}\"\n    }\n}"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(ws, 'AML_Automation_RemotePipelineTraining')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3a23c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3bd3e6e",
   "metadata": {},
   "source": [
    "## Publish Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b821664",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'Diabetes Training Pipeline',\n",
    "                                     description = 'Pipeline that generates batch predictions using a registered trained model.',\n",
    "                                     continue_on_step_failure = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17a5269",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9591b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.pipeline.core import ScheduleRecurrence, Schedule\n",
    "\n",
    "# # Submit the Pipeline every Monday at 00:00 UTC\n",
    "# recurrence = ScheduleRecurrence(frequency=\"Week\", interval=1, week_days=[\"Monday\"], time_of_day=\"00:00\")\n",
    "# weekly_schedule = Schedule.create(ws, name=\"weekly-diabetes-training\", \n",
    "#                                   description=\"Based on time\",\n",
    "#                                   pipeline_id=published_pipeline.id, \n",
    "#                                   experiment_name='mslearn-diabetes-pipeline', \n",
    "#                                   recurrence=recurrence)\n",
    "# print('Pipeline scheduled.')"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
