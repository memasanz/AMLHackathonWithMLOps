{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1482d0b",
   "metadata": {},
   "source": [
    "## MLOps with Azure ML Pipelines\n",
    "\n",
    "ML Pipeline - Training & Registration.  ML Pipelines can help you to build, optimize and manage your machine learning workflow. \n",
    "\n",
    "ML Pipelines encapsulate a workflow for a machine learning task.  Tasks often include:\n",
    "- Data Prep\n",
    "- Training \n",
    "- Publishing Models\n",
    "- Deployment of Models\n",
    "\n",
    "First we will set some key variables to be leveraged inside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2ed2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_env_name = \"experiment_env\"\n",
    "experiment_folder = 'exp_train_pipeline'\n",
    "dataset_prefix_name = 'exp'\n",
    "cluster_name = \"mm-cluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23e496",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8006de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a17a44c",
   "metadata": {},
   "source": [
    "### Connect to the workspace and create a cluster for running the AML Pipeline\n",
    "\n",
    "Connect to the AML workspace and the default datastore. To run an AML Pipeline, we will want to create compute if a compute cluster is not already available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3f59e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a800a4a0",
   "metadata": {},
   "source": [
    "## Create Run configuration\n",
    "\n",
    "The RunConfiguration defines the environment used across all the python steps.  There are a variety of ways of setting up an environment.  An environment holds the required python packages needed for your code to execute on a compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142a17dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_train_pipeline\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da7b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda_yml_file = '../configuration/environment.yml'\n",
    "conda_yml_file = './'+ experiment_folder+ '/environment.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d32e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./exp_train_pipeline/environment.yml\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b44524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "env = Environment.from_conda_specification(\"experiment_env\", conda_yml_file)\n",
    "\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = env\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af1db661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'experiment_env'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registered_env_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7778ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run configuration created.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "experiment_env = Environment.from_conda_specification(registered_env_name, conda_yml_file)\n",
    "\n",
    "# Register the environment \n",
    "experiment_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, registered_env_name)\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "pipeline_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "pipeline_run_config.target = pipeline_cluster\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "pipeline_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51624940",
   "metadata": {},
   "source": [
    "## Define Output datasets\n",
    "\n",
    "\n",
    "The **OutputFileDatasetConfig** object is a special kind of data reference that is used for interim storage locations that can be passed between pipeline steps, so you'll create one and use at as the output for the first step and the input for the second step. Note that you need to pass it as a script argument so your code can access the datastore location referenced by the data reference. \n",
    "\n",
    "Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the register_on_complete() call.\n",
    "\n",
    "These can be viewed in the Datasets tab directly in the AML Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20c4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from storage location and save to exp_raw_data\n",
    "exp_raw_data       = OutputFileDatasetConfig(name='Exp_Raw_Data', destination=(default_ds, dataset_prefix_name + '_raw_data/{run-id}')).read_delimited_files().register_on_complete(name= dataset_prefix_name + '_Raw_Data')\n",
    "\n",
    "#data split into testing and training\n",
    "exp_training_data  = OutputFileDatasetConfig(name='Exp_Training_Data', destination=(default_ds, dataset_prefix_name + '_training_data/{run-id}')).read_delimited_files().register_on_complete(name=dataset_prefix_name + '_Training_Data')\n",
    "exp_testing_data   = OutputFileDatasetConfig(name='Exp_Testing_Data', destination=(default_ds, dataset_prefix_name + '_testing_data/{run-id}')).read_delimited_files().register_on_complete(name=dataset_prefix_name + '_Testing_Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22765c",
   "metadata": {},
   "source": [
    "## Define Pipeline Data\n",
    "\n",
    "Data used in pipeline can be **produced by one step** and **consumed in another step** by providing a PipelineData object as an output of one step and an input of one or more subsequent steps\n",
    "\n",
    "This can be leveraged for moving a model from one step into another for model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c62ebc",
   "metadata": {},
   "source": [
    "### Create Python Script Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2dd4fbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_step = PythonScriptStep(\n",
    "    name='Get Data',\n",
    "    script_name='get_data.py',\n",
    "    arguments =['--exp_raw_data', exp_raw_data],\n",
    "    outputs=[exp_raw_data],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ba2388",
   "metadata": {},
   "source": [
    "### Split Data Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3159a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_scale_step = PythonScriptStep(\n",
    "    name='Split  Raw Data',\n",
    "    script_name='split.py',\n",
    "    arguments =['--exp_training_data', exp_training_data,\n",
    "                '--exp_testing_data', exp_testing_data],\n",
    "    inputs=[exp_raw_data.as_input(name='Exp_Raw_Data')],\n",
    "    outputs=[exp_training_data, exp_testing_data],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0f0e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TrainingStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a62d0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw data will be preprocessed and registered as train/test datasets\n",
    "\n",
    "model_file = PipelineData(name='model_file', datastore=default_ds)\n",
    "\n",
    "#by specifying as input, it does not need to be included in the arguments\n",
    "train_model_step = PythonScriptStep(\n",
    "    name='Train',\n",
    "    script_name='train.py',\n",
    "    arguments =['--model_file_output', model_file],\n",
    "    inputs=[\n",
    "            exp_training_data.as_input(name='Exp_Training_Data'),\n",
    "            exp_testing_data.as_input(name='Exp_Testing_Data'),\n",
    "           ],\n",
    "    outputs = [model_file],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c9362",
   "metadata": {},
   "source": [
    "### Evaluate Model Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5c5b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate and register model here\n",
    "#Compare metrics from current model and register if better than current\n",
    "#best model\n",
    "\n",
    "\n",
    "deploy_file = PipelineData(name='deploy_file', datastore=default_ds)\n",
    "\n",
    "evaluate_and_register_step = PythonScriptStep(\n",
    "    name='Evaluate and Register Model',\n",
    "    script_name='evaluate_and_register.py',\n",
    "    arguments=[\n",
    "        '--model_file', model_file,\n",
    "        '--deploy_file_output', deploy_file,       \n",
    "    ],\n",
    "    inputs=[model_file.as_input('model_file'),\n",
    "            exp_training_data.as_input(name='Exp_Training_Data'),\n",
    "            exp_testing_data.as_input(name='Exp_Testing_Data')\n",
    "           ],\n",
    "    outputs=[ deploy_file],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c1d8c",
   "metadata": {},
   "source": [
    "### Deploy ACI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "359a7d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_deploy_pipeline_data = PipelineData(\n",
    "#         name='scoring_url_file', \n",
    "#         pipeline_output_name='scoring_url_file',\n",
    "#         datastore=default_ds,\n",
    "#         output_mode='mount',\n",
    "#         is_directory=False)\n",
    "\n",
    "scoring_file = PipelineData(name='scoring_file', datastore=default_ds)\n",
    "\n",
    "aci_service_name = 'diabetes-service-remote-training'\n",
    "registered_model_name = 'diabetes_model_remote'\n",
    "\n",
    "env_name = PipelineParameter(name='environment_name', default_value=registered_env_name)\n",
    "service_name = PipelineParameter(name='service_name', default_value=aci_service_name)\n",
    "model_name = PipelineParameter(name='model_name', default_value=registered_model_name)\n",
    "\n",
    "\n",
    "\n",
    "deploy_test = PythonScriptStep(\n",
    "    name='Deploy to ACI',\n",
    "    script_name='deployACI.py',\n",
    "    arguments=[\n",
    "        '--scoring_file_output', scoring_file,\n",
    "        '--deploy_file', deploy_file,\n",
    "        '--environment_name', env_name,\n",
    "        '--service_name', service_name,\n",
    "        '--model_name', model_name\n",
    "        \n",
    "    ],\n",
    "    inputs=[\n",
    "        deploy_file.as_input('deploy_file'),\n",
    "            \n",
    "    ],\n",
    "    outputs=[scoring_file],\n",
    "    compute_target=pipeline_cluster,\n",
    "    source_directory='./' + experiment_folder,\n",
    "    allow_reuse=False,\n",
    "    runconfig=pipeline_run_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115753b",
   "metadata": {},
   "source": [
    "## Create Pipeline steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e261a",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "058aa1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[get_data_step, split_scale_step, train_model_step, evaluate_and_register_step, deploy_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d2c85c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted PipelineRun 18ab20d5-6ff5-4421-8512-38894ae7ea0b\n",
      "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/18ab20d5-6ff5-4421-8512-38894ae7ea0b?wsid=/subscriptions/5da07161-3770-4a4b-aa43-418cbbb627cf/resourcegroups/mm-aml-dev-ops-rg/workspaces/mm-aml-dev-ops&tid=72f988bf-86f1-41af-91ab-2d7cd011db47\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(ws, 'AML_Automation_DevOpsPipelineTraining')\n",
    "run = experiment.submit(pipeline)\n"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
