{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1482d0b",
   "metadata": {},
   "source": [
    "## MLOps with Azure ML Pipelines\n",
    "\n",
    "ML Pipeline - Write Scripts to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2ed2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_env_name = \"experiment_env\"\n",
    "experiment_folder = 'devOps_deploy_pipeline'\n",
    "dataset_prefix_name = 'exp'\n",
    "cluster_name = \"mm-cluster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23e496",
   "metadata": {},
   "source": [
    "Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8006de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f59e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing cluster, use it.\n"
     ]
    }
   ],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        pipeline_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "142a17dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "devOps_deploy_pipeline\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Create a folder for the pipeline step files\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "\n",
    "print(experiment_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1da7b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda_yml_file = '../configuration/environment.yml'\n",
    "conda_yml_file = './'+ experiment_folder+ '/environment.yml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62d32e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./devOps_deploy_pipeline/environment.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile $conda_yml_file\n",
    "name: experiment_env\n",
    "dependencies:\n",
    "- python=3.6.2\n",
    "- scikit-learn\n",
    "- ipykernel\n",
    "- matplotlib\n",
    "- pandas\n",
    "- pip\n",
    "- pip:\n",
    "  - azureml-defaults\n",
    "  - pyarrow\n",
    "  - azureml-monitoring\n",
    "  - azureml-interpret\n",
    "  - inference-schema\n",
    "  - joblib\n",
    "  - azure-ml-api-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f056c71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./devOps_deploy_pipeline/deployACI.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$experiment_folder/deployACI.py\n",
    "\n",
    "import argparse\n",
    "from azureml.core import Workspace, Environment\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.run import Run\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import Webservice, AciWebservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Deploy arg parser')\n",
    "parser.add_argument('--scoring_file_output', type=str, help='File storing the scoring url')\n",
    "parser.add_argument('--deploy_file', type=str, help='File storing if model should be deployed')\n",
    "parser.add_argument('--environment_name', type=str,help='Environment name')\n",
    "parser.add_argument('--service_name', type=str,help='service name')\n",
    "parser.add_argument('--model_name', type=str,help='model name')\n",
    "\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "scoring_url_file = args.scoring_file_output\n",
    "deploy_file      = args.deploy_file\n",
    "environment_name = args.environment_name\n",
    "service_name     = args.service_name\n",
    "model_name       = args.model_name\n",
    "\n",
    "\n",
    "run = Run.get_context()\n",
    "\n",
    "#Get associated AML workspace\n",
    "ws = run.experiment.workspace\n",
    "\n",
    "model = Model(ws, model_name)\n",
    "env = Environment.get(ws, environment_name)\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "\n",
    "# Deploy model\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "            cpu_cores = 1, \n",
    "            memory_gb = 2, \n",
    "            tags = {'model': 'diabetes remote training'},\n",
    "            auth_enabled=True,\n",
    "            enable_app_insights=True,\n",
    "            collect_model_data=True)\n",
    "\n",
    "try:\n",
    "    service = Webservice(ws, name=service_name)\n",
    "    if service:\n",
    "        service.delete()\n",
    "except WebserviceException as e:\n",
    "         print()\n",
    "\n",
    "service = Model.deploy(ws, service_name, [model], inference_config, aci_config)\n",
    "service.wait_for_deployment(True)\n",
    "    \n",
    "\n",
    "# Output scoring url\n",
    "print(service.scoring_uri)\n",
    "with open(scoring_url_file, 'w+') as f:\n",
    "    f.write(service.scoring_uri)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f7162a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./devOps_deploy_pipeline/score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./$experiment_folder/score.py\n",
    "\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "from azureml.core.model import Model\n",
    "from azureml.monitoring import ModelDataCollector\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#version 2\n",
    "# Called when the service is loaded\n",
    "def init():\n",
    "    global model\n",
    "    #Print statement for appinsights custom traces:\n",
    "    print (\"model initialized\" + time.strftime(\"%H:%M:%S\"))\n",
    "    # Get the path to the deployed model file and load it\n",
    "    path = os.path.join(Model.get_model_path('diabetes_model_remote'))\n",
    "    \n",
    "    print(path)\n",
    "    model = joblib.load(path)\n",
    "\n",
    "    \n",
    "    global inputs_dc, prediction_dc\n",
    "    inputs_dc = ModelDataCollector(\"best_model\", designation=\"inputs\", feature_names=['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age'])\n",
    "    prediction_dc = ModelDataCollector(\"best_model\", designation=\"predictions\", feature_names=[\"Diabetic\"])\n",
    "\n",
    "\n",
    "\n",
    "# Called when a request is received\n",
    "def run(raw_data):\n",
    "    try:\n",
    "        # Get the input data as a numpy array\n",
    "        #data = np.array(json.loads(raw_data)['data'])\n",
    "        # Get a prediction from the model\n",
    "        \n",
    "        json_data = json.loads(raw_data)\n",
    "        predictions = model.predict(json_data['data'])\n",
    "        print (\"Prediction created\" + time.strftime(\"%H:%M:%S\"))\n",
    "        # Get the corresponding classname for each prediction (0 or 1)\n",
    "        classnames = ['not-diabetic', 'diabetic']\n",
    "        predicted_classes = []\n",
    "        for prediction in predictions:\n",
    "            val = int(prediction)\n",
    "            predicted_classes.append(classnames[val])\n",
    "        # Return the predictions as JSON\n",
    "        \n",
    "         # Log the input and output data to appinsights:\n",
    "        info = {\n",
    "            \"input\": raw_data,\n",
    "            \"output\": predicted_classes\n",
    "            }\n",
    "        print(json.dumps(info))\n",
    "        \n",
    "        inputs_dc.collect(json_data['data']) #this call is saving our input data into Azure Blob\n",
    "        prediction_dc.collect(predicted_classes) #this call is saving our prediction data into Azure Blob\n",
    "\n",
    "        \n",
    "        return json.dumps(predicted_classes)\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        print (error + time.strftime(\"%H:%M:%S\"))\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b549cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
